# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PJKRwgp4ButkQI5fGShBTNYeI6CAXku3
"""

# 1. Upload the Dataset
from google.colab import files
uploaded = files.upload()

# 2. Load the Dataset
import pandas as pd

df = pd.read_csv("fake_and_real_news.csv")  # Adjust filename if needed
df.head()

# 3. Data Exploration
df.info()  # Overview of data types and non-null values
df.describe()  # Summary statistics for numerical columns
df.columns  # List of column names

# 4. Check for Missing Values and Duplicates
print("Missing values:\n", df.isnull().sum())
print("\nDuplicate rows:", df.duplicated().sum())

# 5. Visualize a Few Features
import matplotlib.pyplot as plt
import seaborn as sns

# Example: Distribution of a categorical feature
sns.countplot(x='label', data=df)
plt.title('Distribution of News Labels')
plt.show()

# 6. Identify Target and Features
# Assuming 'label' is the target (fake or real)
X = df.drop(columns=['label'])  # Features
y = df['label']  # Target

# 7. Convert Categorical Columns to Numerical
# For example, convert text features like 'title' or 'text' to numerical (if needed for simple models)
# We'll just encode the label here for a basic binary classifier
from sklearn.preprocessing import LabelEncoder

label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)  # 'fake' -> 0, 'real' -> 1

# 8. One-Hot Encoding (if you have categorical features like 'author', 'subject')
# Example:
# X = pd.get_dummies(X, columns=['author', 'subject'])

# 9. Feature Scaling
from sklearn.preprocessing import StandardScaler
from sklearn.feature_extraction.text import TfidfVectorizer

# Use TF-IDF for text data
tfidf = TfidfVectorizer(stop_words='english', max_df=0.7)
# Accessing the column using 'Text' instead of 'text'
X_tfidf = tfidf.fit_transform(df['Text'].fillna(""))

# Optional: Scale numerical features
# scaler = StandardScaler()
# X_scaled = scaler.fit_transform(X_numerical)

# 10. Train-Test Split
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)

# 11. Model Building
from sklearn.linear_model import LogisticRegression

model = LogisticRegression()
model.fit(X_train, y_train)

# 13. Make Predictions from New Input
sample_text = ["Breaking news! Scientists discover new planet."]  # Sample input
sample_vec = tfidf.transform(sample_text)
prediction = model.predict(sample_vec)
print("Prediction:", label_encoder.inverse_transform(prediction))

# 14. Convert to DataFrame and Encode
# Not required here unless you're batch predicting on a new DataFrame

# 15. Predict the Final Grade
# Already handled in step 13 as predicting the news label

# 16. Deployment - Building an Interactive App
!pip install gradio

# 17. Create a Prediction Function
import gradio as gr

def predict_news(text):
    vec = tfidf.transform([text])
    pred = model.predict(vec)
    return label_encoder.inverse_transform(pred)[0]

# 18. Create the Gradio Interface
interface = gr.Interface(fn=predict_news,
                         inputs="text",
                         outputs="text",
                         title="ðŸŽ“ Student Performance Predictor",
                         description="Enter a news article text to predict if it's Fake or Real")
interface.launch()